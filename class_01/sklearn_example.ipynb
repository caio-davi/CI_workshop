{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to import some libraries. As you can notice below, lots of classifiers are being imported to our code, but we will not use all of them at the same time. In the next steps, you will understand why we are doing it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics, svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can import the original dataset. We will also visualize the first samples of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('disease.csv', header=0)\n",
    "original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training, the first step should be separate the samples into two different collections. One with all the features and the other one with the classes of each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = original.drop('diagnosis', 1)\n",
    "diag = original.loc[:,['diagnosis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our features in a dataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our the classes of each sample in another dataFrame:\n",
    "> Notice both dataFrames share the same index, so we have a common key to use with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should have noticed, our data isn't numeric, which means that it isn't suitable for most of the techniques discussed here. This trick will enable us to get around this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in snps.columns:\n",
    "\tdf_coded = snps \n",
    "\tsetattr(df_coded, feature, getattr(snps,feature).astype(\"category\").cat.codes)\n",
    "snps.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice: We just mapped the original categories to a numeric value representation. This should be fine for some techniques, but not for all. Some algorithms could be influenced by those values and it can change their behavior. If you want to use an of those algorithms, you can try [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), it should do the trick for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use cross-validation to check the performance of our classifiers. This is by far the most used validator used in Machine Learning, but it has some limitations especially when used with small sample data. As the focus of this workshop isn't the validator performance, we will use a built-in cross validator from Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=2**10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have to create independent datasets for training and testing our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv.split(snps, diag):\n",
    "#   Setting training data\n",
    "    df_train = snps.loc[train]\n",
    "    diag_train = diag.loc[train]\n",
    "#   Setting test data\n",
    "    df_test  = snps.loc[test]\n",
    "    diag_test  = diag.loc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have two sets of data: one to train our classifier and another test it. Let's take a look on they?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can create and train our classifier. We will use a Multilayer Perceptron Neural Network now, but some steps ahead you will see how easy it will be to change the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8, 5))\n",
    "clf.fit(df_train,diag_train.pop('diagnosis').tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can predict our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(df_test)\n",
    "#   Formating before comparing \n",
    "diag_test_ = diag_test.pop('diagnosis').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(diag_test_, predicted))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(diag_test_, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can go back and select different classifiers and parameters to test your data. Try to run the next code snippet uncommenting different classifiers and changing their parameters. [Here](https://scikit-learn.org/stable/modules/classes.html) you can find  the documentation to help you with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv.split(snps, diag):\n",
    "    df_train = snps.loc[train]\n",
    "    diag_train = diag.loc[train]\n",
    "    df_test  = snps.loc[test]\n",
    "    diag_test  = diag.loc[test]\n",
    "\n",
    "# clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "# clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "# clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8, 5))\n",
    "# clf = RandomForestClassifier(n_estimators=100)\n",
    "# clf = AdaBoostClassifier(n_estimators=100)\n",
    "# clf = NearestCentroid()\n",
    "\n",
    "clf.fit(df_train,diag_train.pop('diagnosis').tolist())\n",
    "predicted = clf.predict(df_test)\n",
    "diag_test = diag_test.pop('diagnosis').tolist()\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(diag_test, predicted))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(diag_test, predicted))\n",
    "print(\"Classifier: \", clf  ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
